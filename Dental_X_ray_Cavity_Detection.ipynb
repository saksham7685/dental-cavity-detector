{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mxemh3-9zynd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CyKtd37q3tt3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#  Configuration\n",
    "DATA_ROOT = 'C:\\\\Users\\\\91741\\\\Downloads\\\\dental_xray' # Path to dataset\n",
    "NUM_CLASSES = 2  # 1 (cavity) + 1 (background)\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 8\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hgDES_rX37v-"
   },
   "outputs": [],
   "source": [
    "def read_mask_image(mask_path):\n",
    "    \"\"\"\n",
    "    Reads a mask image and converts it into a binary mask tensor.\n",
    "    Assumes mask images are grayscale where non-zero pixels represent the cavity.\n",
    "    \"\"\"\n",
    "    mask = Image.open(mask_path).convert(\"L\") # Convert to grayscale\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    # Convert to binary mask: 1 for cavity, 0 for background\n",
    "    # Thresholding might be needed if masks are not perfectly binary (e.g., 0-255 values)\n",
    "    mask[mask > 0] = 1 # Assuming any non-zero value is a cavity\n",
    "    mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j3j-Nca94BNB"
   },
   "outputs": [],
   "source": [
    "def get_bounding_box_from_mask(mask):\n",
    "    \"\"\"\n",
    "    Generates bounding box coordinates from a binary mask.\n",
    "    Handles cases where a mask might contain multiple disconnected components,\n",
    "    treating each as a separate instance.\n",
    "    \"\"\"\n",
    "    # Find contours in the mask\n",
    "    # cv2.RETR_EXTERNAL retrieves only the extreme outer contours.\n",
    "    # cv2.CHAIN_APPROX_SIMPLE compresses horizontal, vertical, and diagonal segments.\n",
    "    contours, _ = cv2.findContours(mask.numpy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    boxes = []\n",
    "    for contour in contours:\n",
    "        # Get bounding box for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        boxes.append([x, y, x + w, y + h])\n",
    "\n",
    "    if not boxes:\n",
    "        # If no contours found (e.g., empty mask), return a dummy box\n",
    "        # This is important to avoid errors in Mask R-CNN training if some masks are truly empty\n",
    "        return torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YxPQbFuQ4G7u"
   },
   "outputs": [],
   "source": [
    "class DentalXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading dental X-ray images and their corresponding\n",
    "    cavity masks and bounding box annotations.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_dir = os.path.join(root_dir, 'def_images')\n",
    "        self.mask_dir = os.path.join(root_dir, 'mask_images')\n",
    "\n",
    "        # List all image files. Assuming image names are consistent with mask names.\n",
    "        self.image_filenames = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        # --- MORE ROBUST MASK NAME GENERATION AND CHECK ---\n",
    "        base_name, img_ext = os.path.splitext(img_name)\n",
    "        possible_mask_paths = []\n",
    "\n",
    "        # 1. Try exact same name and extension (e.g., images_7666.png -> images_7666.png)\n",
    "        possible_mask_paths.append(os.path.join(self.mask_dir, img_name))\n",
    "\n",
    "        # 2. Try with '_mask.png' suffix (e.g., images_7666.png -> images_7666_mask.png)\n",
    "        possible_mask_paths.append(os.path.join(self.mask_dir, f\"{base_name}_mask.png\"))\n",
    "\n",
    "        # 4. Try exact same name but different common image extensions for masks\n",
    "        if img_ext.lower() == '.jpg':\n",
    "            possible_mask_paths.append(os.path.join(self.mask_dir, f\"{base_name}.png\"))\n",
    "        elif img_ext.lower() == '.png':\n",
    "            possible_mask_paths.append(os.path.join(self.mask_dir, f\"{base_name}.jpg\"))\n",
    "\n",
    "\n",
    "        mask_path = None\n",
    "        for p in possible_mask_paths:\n",
    "            if os.path.exists(p):\n",
    "                mask_path = p\n",
    "                break\n",
    "\n",
    "        if mask_path is None:\n",
    "            # If no mask file is found after trying all common patterns, raise an error\n",
    "            raise FileNotFoundError\n",
    "        # --- END ROBUST MASK NAME GENERATION AND CHECK ---\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\") # Ensure image is in RGB format\n",
    "\n",
    "        # Load mask and convert to binary mask tensor\n",
    "        mask = read_mask_image(mask_path) # This is a single mask image for the whole image\n",
    "\n",
    "        # Get bounding boxes from the mask.\n",
    "        boxes = get_bounding_box_from_mask(mask)\n",
    "\n",
    "        # Create labels: 1 for cavity (foreground)\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        # Find connected components in the mask to get individual instance masks\n",
    "        num_labels, labeled_mask = cv2.connectedComponents(mask.numpy())\n",
    "        instance_masks = []\n",
    "        for i in range(1, num_labels): # Iterate through each detected component (skip background 0)\n",
    "            instance_mask = (labeled_mask == i).astype(np.uint8)\n",
    "            instance_masks.append(torch.as_tensor(instance_mask, dtype=torch.uint8))\n",
    "\n",
    "        if not instance_masks:\n",
    "            instance_masks = torch.zeros((0, mask.shape[0], mask.shape[1]), dtype=torch.uint8)\n",
    "        else:\n",
    "            instance_masks = torch.stack(instance_masks)\n",
    "\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = instance_masks\n",
    "        target[\"image_id\"] = torch.tensor([idx]) # Unique ID for the image\n",
    "        target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # Area of the bounding box\n",
    "        target[\"iscrowd\"] = torch.zeros((boxes.shape[0],), dtype=torch.int64) # All instances are not crowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            # Apply transforms. The Compose class is now updated to handle this correctly.\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qWCbDBVO4WO8"
   },
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    This custom Compose is simplified and only applies F.to_tensor to the image.\n",
    "    For augmentations that also affect bounding boxes and masks,\n",
    "    use `torchvision.transforms.v2.Compose`.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Apply transforms. F.to_tensor only applies to the image.\n",
    "        # The target dictionary is passed through unchanged.\n",
    "        for t in self.transforms:\n",
    "            # F.to_tensor is a functional transform that only takes the image.\n",
    "            # Other transforms (like geometric augmentations in v2) would handle target too.\n",
    "            image = t(image) # Apply transform to image\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6lTKEmDu4K2p"
   },
   "outputs": [],
   "source": [
    "# --- Transformations ---\n",
    "def get_transform(train):\n",
    "    \"\"\"\n",
    "    Defines image transformations for training and validation.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    transforms.append(F.to_tensor) # Convert PIL Image to PyTorch Tensor\n",
    "\n",
    "    if train:\n",
    "        # Add data augmentation for training\n",
    "        # Example: Random horizontal flip\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uW2UvP8d4RLe"
   },
   "outputs": [],
   "source": [
    "# --- Model Definition ---\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained Mask R-CNN model and modifies its head\n",
    "    for the specific number of classes.\n",
    "    \"\"\"\n",
    "    # Load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Get the number of input features for the mask predictor\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256 # A common choice for the hidden layer size\n",
    "    # Replace the pre-trained mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XcYRSImO4cK1"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    \"\"\"\n",
    "    Conceptual training loop for one epoch.\n",
    "    In a real scenario, you would calculate losses, perform backpropagation,\n",
    "    and update model weights here.\n",
    "    \"\"\"\n",
    "    model.train() # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over the data loader\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass: model returns a dictionary of losses\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Sum up all losses\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        losses.backward()     # Compute gradients\n",
    "        optimizer.step()      # Update weights\n",
    "\n",
    "        if i % 10 == 0: # Print loss every 10 batches\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {losses.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch} finished. Average Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rrVZkiec4jF-"
   },
   "outputs": [],
   "source": [
    "def visualize_prediction(image_path, model, device, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Performs inference on a single image and visualizes the predicted\n",
    "    bounding boxes and segmentation masks.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img).to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        prediction = model([img_tensor])\n",
    "\n",
    "    # Convert image back to numpy for visualization\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(\"Predicted Cavities\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Process predictions\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().numpy()\n",
    "    masks = prediction[0]['masks'].cpu().numpy() # Masks are (N, 1, H, W)\n",
    "\n",
    "    # Filter predictions based on score threshold\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] > threshold:\n",
    "            box = boxes[i]\n",
    "            label = labels[i]\n",
    "            score = scores[i]\n",
    "            mask = masks[i, 0] # Take the first channel of the mask (binary mask)\n",
    "\n",
    "            # Draw bounding box\n",
    "            rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
    "                                     linewidth=2, edgecolor='r', facecolor='none',\n",
    "                                     label=f'Cavity: {score:.2f}')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Apply mask overlay\n",
    "            # Convert mask to binary (0 or 1) and resize to original image dimensions if necessary\n",
    "            mask = (mask > 0.5).astype(np.uint8) # Threshold mask to binary\n",
    "            # If mask is smaller than original image, resize it (Mask R-CNN outputs masks at 28x28 by default, then upsamples)\n",
    "            # Ensure mask is the same size as the image for overlay\n",
    "            if mask.shape[0] != img_np.shape[0] or mask.shape[1] != img_np.shape[1]:\n",
    "                mask = cv2.resize(mask, (img_np.shape[1], img_np.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            # Create a colored overlay for the mask\n",
    "            color_mask = np.zeros_like(img_np)\n",
    "            color_mask[mask == 1] = [255, 0, 0] # Red color for cavity mask\n",
    "\n",
    "            # Blend the mask with the original image\n",
    "            # alpha controls the transparency of the mask (e.g., 0.5 for 50% transparency)\n",
    "            ax.imshow(color_mask, alpha=0.4)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "ghxhXIdw4mtW",
    "outputId": "2d01e3c6-b286-4403-b436-e83dce6e447f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Training samples: 240, Validation samples: 60\n",
      "Initializing Mask R-CNN model\n",
      "This code will train for 8 epochs.\n",
      "\n",
      "Epoch 1/8\n",
      "Epoch: 0, Batch: 0, Loss: 8.1458\n",
      "Epoch: 0, Batch: 10, Loss: 1.0486\n",
      "Epoch: 0, Batch: 20, Loss: 1.5216\n",
      "Epoch: 0, Batch: 30, Loss: 1.3281\n",
      "Epoch: 0, Batch: 40, Loss: 1.2826\n",
      "Epoch: 0, Batch: 50, Loss: 0.9276\n",
      "Epoch: 0, Batch: 60, Loss: 0.7774\n",
      "Epoch: 0, Batch: 70, Loss: 1.0983\n",
      "Epoch: 0, Batch: 80, Loss: 0.7748\n",
      "Epoch: 0, Batch: 90, Loss: 0.9026\n",
      "Epoch: 0, Batch: 100, Loss: 0.7977\n",
      "Epoch: 0, Batch: 110, Loss: 0.8276\n",
      "Epoch 0 finished. Average Loss: 1.2639\n",
      "\n",
      "Epoch 2/8\n",
      "Epoch: 1, Batch: 0, Loss: 0.9894\n",
      "Epoch: 1, Batch: 10, Loss: 0.6987\n",
      "Epoch: 1, Batch: 20, Loss: 0.8194\n",
      "Epoch: 1, Batch: 30, Loss: 0.5716\n",
      "Epoch: 1, Batch: 40, Loss: 3.3626\n",
      "Epoch: 1, Batch: 50, Loss: 0.7859\n",
      "Epoch: 1, Batch: 60, Loss: 0.7635\n",
      "Epoch: 1, Batch: 70, Loss: 0.7549\n",
      "Epoch: 1, Batch: 80, Loss: 0.9237\n",
      "Epoch: 1, Batch: 90, Loss: 0.7735\n",
      "Epoch: 1, Batch: 100, Loss: 0.6959\n",
      "Epoch: 1, Batch: 110, Loss: 0.8464\n",
      "Epoch 1 finished. Average Loss: 1.0125\n",
      "\n",
      "Epoch 3/8\n",
      "Epoch: 2, Batch: 0, Loss: 0.6165\n",
      "Epoch: 2, Batch: 10, Loss: 0.6319\n",
      "Epoch: 2, Batch: 20, Loss: 0.8775\n",
      "Epoch: 2, Batch: 30, Loss: 3.2624\n",
      "Epoch: 2, Batch: 40, Loss: 0.6167\n",
      "Epoch: 2, Batch: 50, Loss: 0.7012\n",
      "Epoch: 2, Batch: 60, Loss: 0.7265\n",
      "Epoch: 2, Batch: 70, Loss: 0.7413\n",
      "Epoch: 2, Batch: 80, Loss: 0.6739\n",
      "Epoch: 2, Batch: 90, Loss: 0.7235\n",
      "Epoch: 2, Batch: 100, Loss: 0.8344\n",
      "Epoch: 2, Batch: 110, Loss: 0.8444\n",
      "Epoch 2 finished. Average Loss: 0.9495\n",
      "\n",
      "Epoch 4/8\n",
      "Epoch: 3, Batch: 0, Loss: 0.6479\n",
      "Epoch: 3, Batch: 10, Loss: 0.7674\n",
      "Epoch: 3, Batch: 20, Loss: 0.6944\n",
      "Epoch: 3, Batch: 30, Loss: 0.8360\n",
      "Epoch: 3, Batch: 40, Loss: 0.7303\n",
      "Epoch: 3, Batch: 50, Loss: 0.6406\n",
      "Epoch: 3, Batch: 60, Loss: 0.7877\n",
      "Epoch: 3, Batch: 70, Loss: 0.6434\n",
      "Epoch: 3, Batch: 80, Loss: 0.5964\n",
      "Epoch: 3, Batch: 90, Loss: 0.7022\n",
      "Epoch: 3, Batch: 100, Loss: 0.6749\n",
      "Epoch: 3, Batch: 110, Loss: 0.9010\n",
      "Epoch 3 finished. Average Loss: 0.9103\n",
      "\n",
      "Epoch 5/8\n",
      "Epoch: 4, Batch: 0, Loss: 1.6501\n",
      "Epoch: 4, Batch: 10, Loss: 0.6418\n",
      "Epoch: 4, Batch: 20, Loss: 0.6808\n",
      "Epoch: 4, Batch: 30, Loss: 0.6077\n",
      "Epoch: 4, Batch: 40, Loss: 0.6423\n",
      "Epoch: 4, Batch: 50, Loss: 0.6477\n",
      "Epoch: 4, Batch: 60, Loss: 0.7055\n",
      "Epoch: 4, Batch: 70, Loss: 1.5705\n",
      "Epoch: 4, Batch: 80, Loss: 0.7305\n",
      "Epoch: 4, Batch: 90, Loss: 0.6901\n",
      "Epoch: 4, Batch: 100, Loss: 0.6186\n",
      "Epoch: 4, Batch: 110, Loss: 0.7432\n",
      "Epoch 4 finished. Average Loss: 0.8677\n",
      "\n",
      "Epoch 6/8\n",
      "Epoch: 5, Batch: 0, Loss: 0.8829\n",
      "Epoch: 5, Batch: 10, Loss: 0.7171\n",
      "Epoch: 5, Batch: 20, Loss: 0.7251\n",
      "Epoch: 5, Batch: 30, Loss: 0.6930\n",
      "Epoch: 5, Batch: 40, Loss: 0.8087\n",
      "Epoch: 5, Batch: 50, Loss: 0.6685\n",
      "Epoch: 5, Batch: 60, Loss: 0.6797\n",
      "Epoch: 5, Batch: 70, Loss: 0.5726\n",
      "Epoch: 5, Batch: 80, Loss: 0.6049\n",
      "Epoch: 5, Batch: 90, Loss: 0.6437\n",
      "Epoch: 5, Batch: 100, Loss: 2.7946\n",
      "Epoch: 5, Batch: 110, Loss: 2.3249\n",
      "Epoch 5 finished. Average Loss: 0.8645\n",
      "\n",
      "Epoch 7/8\n",
      "Epoch: 6, Batch: 0, Loss: 0.6566\n",
      "Epoch: 6, Batch: 10, Loss: 0.6916\n",
      "Epoch: 6, Batch: 20, Loss: 0.6427\n",
      "Epoch: 6, Batch: 30, Loss: 0.6390\n",
      "Epoch: 6, Batch: 40, Loss: 0.6161\n",
      "Epoch: 6, Batch: 50, Loss: 0.5988\n",
      "Epoch: 6, Batch: 60, Loss: 0.7446\n",
      "Epoch: 6, Batch: 70, Loss: 0.6202\n",
      "Epoch: 6, Batch: 80, Loss: 0.7567\n",
      "Epoch: 6, Batch: 90, Loss: 0.6735\n",
      "Epoch: 6, Batch: 100, Loss: 0.6175\n",
      "Epoch: 6, Batch: 110, Loss: 2.8017\n",
      "Epoch 6 finished. Average Loss: 0.8638\n",
      "\n",
      "Epoch 8/8\n",
      "Epoch: 7, Batch: 0, Loss: 0.6672\n",
      "Epoch: 7, Batch: 10, Loss: 0.7048\n",
      "Epoch: 7, Batch: 20, Loss: 0.7211\n",
      "Epoch: 7, Batch: 30, Loss: 0.7858\n",
      "Epoch: 7, Batch: 40, Loss: 0.6597\n",
      "Epoch: 7, Batch: 50, Loss: 0.7746\n",
      "Epoch: 7, Batch: 60, Loss: 0.6698\n",
      "Epoch: 7, Batch: 70, Loss: 0.6980\n",
      "Epoch: 7, Batch: 80, Loss: 0.5838\n",
      "Epoch: 7, Batch: 90, Loss: 0.5199\n",
      "Epoch: 7, Batch: 100, Loss: 2.4055\n",
      "Epoch: 7, Batch: 110, Loss: 0.7995\n",
      "Epoch 7 finished. Average Loss: 0.8552\n",
      "Model weights saved  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Preparing dataset and configuring test train split\n",
    "    dataset = DentalXrayDataset(DATA_ROOT, get_transform(train=True))\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Define a collate_fn for the DataLoader, as targets are dictionaries\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"Dataset loaded. Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "    # Get the Model\n",
    "    print(\"Initializing Mask R-CNN model\")\n",
    "    model = get_model_instance_segmentation(NUM_CLASSES)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    #Define Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n",
    "    # And a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "\n",
    "    print(f\"This code will train for {NUM_EPOCHS} epochs.\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        train_one_epoch(model, optimizer, train_loader, DEVICE, epoch)\n",
    "        lr_scheduler.step() # Update learning rate\n",
    "\n",
    "    # Saving Model \n",
    "    torch.save(model.state_dict(), 'mask_rcnn_cavity_detector.pth')\n",
    "    print(\"Model weights saved  \")\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
